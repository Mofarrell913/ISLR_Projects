---
title: "Support Vector Machines Lab"
output: html_notebook
---

This is the lab section for ISLR's Chapter 9

The e1071 library contains implementations for a number of statistical learning methods. In particular, the svm()
function can be used to fit a support vector classifier when the argument kernel="linear" is used. 

A cost argument allows us to specify the cost of a violation to the margin. When the cost argument is small,
then the margins will be wide and many support vectors will be on the margin or will violate the margin.

We now use the svm() function to fit the support vector classifier for a given value of the cost parameter.

```{r}
# Create two nonseperable classes
set.seed(1)
x = matrix(rnorm(20*2), ncol = 2)
y = c(rep(-1,10), rep(1,10))
x[y==1,] = x[y==1,] + 1
plot(x, col = (3-y))

# Fit a support vector classifier 
# Scale = False does not apply feature scaling
dat = data.frame(x=x,y=as.factor(y))
library(e1071)
svmfit = svm(y~., data = dat, kernel = "linear", cost = .1, scale = FALSE)

# Two arguments - svm fit and the data used in the call of svmfit
plot(svmfit, dat)

# Identities of support vectors
svmfit$index

# Output information regarding the kernal, cost, gamma value, and support vectors
summary(svmfit)

# Determining the cost parameter with the tune function
# By default, tune() performs ten-fold cross-validation on a set of models of interest.
set.seed(1)
tune.out = tune(svm, y~., data = dat, kernal = "linear", ranges = list(cost= c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
# Choose best model
bestmod = tune.out$best.model
summary(bestmod)
```

Making predictions with the best model
```{r}

xtest = matrix(rnorm(20*2),ncol = 2)
ytest = sample(c(-1,1),20,replace = TRUE)
xtest[ytest==1,] = xtest[ytest == 1,] + 1
testdat = data.frame(x=xtest, y= as.factor(ytest))
ypred = predict(bestmod, testdat)
table(predict=ypred, truth = testdat$y)

# using a different classifier tuning parameter
svmfit = svm(y~., data = dat, kernel = "linear", cost=.01, scale = FALSE)
ypred = predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
```

Creating observations that are barely linearly separable
```{r}
x[y==1,] = x[y==1,] + .5
plot(x,col=(y+5)/2,pch=19)

# create SVC so with a large value of cost so nothing is misclassified
# No training errors were made, and only 3 support vectors were used
# A highly variable fit
dat = data.frame(x=x,y=as.factor(y))
svmfit = svm(y~., data = dat, kernel = "linear", cost = 1e5)
summary(svmfit)

# by decreasing cost, our variance decreases, but we misclassify one of the observations, but seven support vectors were used
svmfit = svm(y~.,data = dat, kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit,dat)
```

For SVMs with a nonlinear keynel, we can alter the kernel parameter
```{r}
#Generate data with a nonlinear class boundary
x = matrix(rnorm(200*2),ncol=2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] - 2
y = c(rep(1,150),rep(2,50))
dat = data.frame(x=x,y=as.factor(y))
plot(x,col=y)

#train/test split is made, and then we fit svm function with gamma = 1
train = sample(200,100)
svmfit = svm(y~.,data = dat[train,],kernel = "radial", gamma = 1, cost = 1)
plot(svmfit, dat[train,])
summary(svmfit)
```

We see above there's a fair number of trainig errors
```{r}
# we can increase the value of cost to reduce the number of training errors
svmfit = svm(y~.,data = dat[train,], kernel = "radial", gamma = 1, cost = 1e5)
plot(svmfit,dat[train,])

# use tune() to select the best choice of gamma and cost
# Best choice is when cost = 1 and gamma = 1
set.seed(1)
tune.out = tune(svm,y~.,data = dat[train,],kernel = "radial", ranges = list(cost = c(0.1,1,10,100,1000), gamma = c(.5,1,2,3,4)))
summary(tune.out)

# display confusion matrix
table(true = dat[-train,"y"], pred = predict(tune.out$best.model, newdata = dat[-train,]))
```

ROCR Package can be used to produce ROC curves
We first write a short function to plot an ROC curve given a vector containing a numerical score for each observation, pred, and a vector containing the class label for each observation, truth.
```{r}
library(ROCR)
rocplot = function(pred,truth,...) {
  predob = prediction(pred,truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf,...)
}

# to obtain the fitted values for a given SVM model fit, we use decision.values = TRUE when fitting svm(). Predict() will output fitted values
svmfit.opt = svm(y~.,data = dat[train,], kernel = "radial", gamma = 2, cost = 1, devision.values = T)
fitted = attributes(predict(svmfit.opt,dat[train,],decision.values = TRUE))$decision.values

# ROC Plot is produced with training data
par(mfrow = c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")

# By increasing gamma we can produce a more flexible fit
svmfit.flex = svm(y~., data = dat[train,], kernel = "radial", gamma = 50, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.flex,dat[train,],decision.values = TRUE))$decision.values
rocplot(fitted,dat[train,"y"], add = T, col = "red")

# ROC Plot is produced with test data
fitted = attributes(predict(svmfit.opt,dat[-train,],decision.values = TRUE))$decision.values
rocplot(fitted,dat[-train,"y"],main="Test Data")

fitted = attributes(predict(svmfit.flex,dat[-train,],decision.values = TRUE))$decision.values
rocplot(fitted,dat[-train,"y"], add = T, col = "red")
```

SVM with multiple classes
If the response is a factor containing more than two levels, svm() will do one-versus-one approach.
```{r}
# Generate 3 classes of observations
set.seed(1)

x = rbind(x,matrix(rnorm(50*2),ncol=2))
y = c(y,rep(0,50))
x[y==0,2]= x[y==0 ,2]+2
dat=data.frame(x=x, y=as.factor(y))

# Plot decision boundary, and fit svm muliclassifier
par(mfrow=c(1,1))
plot(x,col=(y+1))
svmfit=svm(yâˆ¼., data=dat , kernel ="radial", cost=10, gamma =1)
plot(svmfit , dat)
```


