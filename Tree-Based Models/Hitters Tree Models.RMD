---
title: "Boosting Hitters"
output: html_notebook
---

This is Problem 10 of ISLR's Chapter 8 Applied Section
"We now use boosting to predict Salary in the Hitters data set."

```{r}
library(ISLR)
library(tidyverse)
library(tree)
library(randomForest)
library(gbm)
set.seed(1)
#Part A - Removing NA Observations, and log-transform salaries
Hitters.data = as_tibble(Hitters)
names(Hitters.data)
newHit = Hitters.data %>% drop_na()
newHit$Salary = log10(newHit$Salary)

train = sample(1:nrow(newHit), 200)
test = -train

# Part C, D - As lambda increases, training set MSE decreases, but test set MSE increases
lambda = c(.0001, .0003,.001, .003,.01, .03,.1,.3,1)
errors = rep(0,9)
test.errors = rep(0,9)
for (i in 1:length(lambda))
{
  boost.hit = gbm(Salary~., data = newHit[train,],distribution = "gaussian",n.trees = 1000, shrinkage = lambda[i], verbose = FALSE)
  boost.train = predict(boost.hit, newHit[train,],n.trees = 100)
  boost.test = predict(boost.hit, newHit[test,],n.trees = 100)
  errors[i] = mean((boost.train - newHit$Salary[train])^2)
  test.errors[i] = mean((boost.train - newHit$Salary[test])^2)
}
plot(errors~lambda)
plot(test.errors~lambda)
```


```{r}
library(glmnet)

#Part E - Implement Linear regression and Ridge Regression. When the right shrinkage value is selected, it performs better than both linear and ridge regression
linear = lm(Salary~., data = newHit[train,])
grid = 10^seq(10,-2,length=100)
x = model.matrix(Salary~.,newHit)[,-1]
ridge = glmnet(x[train,],newHit$Salary[train],alpha = 0, lambda = grid,thresh = 1e-12)

linear.pred = predict(linear,newHit[test,])
ridge.pred = predict(ridge, s = 1e-12, newx = x[test,])

mean((linear.pred - newHit$Salary[test])^2)
mean((ridge.pred - newHit$Salary[test])^2)

boost.hit = gbm(Salary~., data = newHit[train,],distribution = "gaussian",n.trees = 1000, shrinkage = .0003, verbose = FALSE)
boost.test = predict(boost.hit, newHit[test,],n.trees = 1000)
mean((boost.test - newHit$Salary[test])^2)

# Part F- CAtBat, CHits, CRuns, and CRBIs seem to be the most important predictors, followed by CWalks
summary(boost.hit)

# Part G - Bagging performs the best of the regression models
bag.hit = randomForest(Salary~.,data = newHit, subset = train, mtry = ncol(newHit)-1,ntree = 100)
bag.pred = predict(bag.hit, newdata = newHit[test,])
mean((bag.pred-newHit$Salary[test])^2)

```

