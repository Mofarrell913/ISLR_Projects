---
title: "ISLR Tree Lab"
output: html_notebook
---

This is the tree-based models lab from ISLR's chapter 8

First, we'll use trees to analyze the Carseats dataset from the ISLR library. We'll split sales, a continuous variable, into high and low sales volumes.

9% is the training error rate of the decision tree.  A small deviance indicates a tree that provides
a good fit to the (training) data. The residual mean deviance reported is
simply the deviance divided by n−|T0|, which in this case is 400−27 = 373.
```{r}
library(tree)
library(ISLR)
library(tidyverse)
High = ifelse(Carseats$Sales<=8,"No","Yes")
Carseats = as_tibble(data.frame(Carseats, High))

#Tree Syntax is similar to that of lm() function
tree.carseats = tree(High~.-Sales, data = Carseats)
summary(tree.carseats)

# The argument pretty=0 instructs R to include the category names 
# for any qualitative predictors, rather than simply displaying a
# letter for each category. text() gives node labels
plot(tree.carseats)
text(tree.carseats,pretty = 0)
```


Now, we approximae the test error of the tree
```{r}
set.seed(2)
train = sample(1:nrow(Carseats),200)
Carseats.test = Carseats[-train,]
High.test = High[-train]
tree.carseats = tree(High~.-Sales,Carseats,subset = train)
tree.pred = predict(tree.carseats, Carseats.test,type="class")
table(tree.pred,High.test)
print((86+57)/200)

```

cv.tree() performs cross-validation to determine the optimal level of tree complexity 
dev corresponds to cv error rate 
```{r}
set.seed(3)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow = c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type = "b")
plot(cv.carseats$k,cv.carseats$dev, type = "b")

#prune.miscalss() punrs the tree to obtain the nine node tree, which has the lowest CV error
prune.carseats = prune.misclass(tree.carseats,best = 9)
plot(prune.carseats)
text(prune.carseats,pretty = 0)

#We then apply the predict function for the test set error
tree.pred = predict(prune.carseats,Carseats.test, type = "class")
table(tree.pred,High.test)
```

Next, we fit a regression tree to the boston data set
In this case, the most complex tree is selected by cross-validation, but if we wanted to prune tree, we could, which is why we're going to
```{r}
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston = tree(medv~.,Boston,subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)

# The tree indicates that lower values of lstat correspond to more expensive houses. The tree predicts a median house price of $46, 400 for larger homes in suburbs in which residents have high socioeconomic status

# Performing cv on the tree
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type = 'b')

#Pruning tree cause why not
prune.boston = prune.tree(tree.boston,best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)

# Predicting results with unpruned tree
yhat = predict(tree.boston, newdata = Boston[-train,])
boston.test = Boston[-train, "medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```

Now we apply bagging and random forests to the Boston data, using randomForest package in R. recall that bagging in random forest when m = p
mtry argument is pretty much m, the subset of predictors
bagged model halves the testMSE that the optimally-pruned single tree had
```{r}
library(randomForest)
set.seed(1)

# Build Bagging model
bag.boston = randomForest(medv~.,data = Boston, subset = train, mtry = 13, importance = TRUE)
bag.boston

#Predict values using the bagging model
yhat.bag = predict(bag.boston,newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)

# changing the number of trees used in the model
bag.boston = randomForest(medv~.,data = Boston, subset = train, mtry = 13, ntree = 25)
yhat.bag = predict(bag.boston,newdata = Boston[-train,])
mean((yhat.bag-boston.test)^2)

```

In growing a random forest, we use a smaller value in the mtry , randomForest does p/3 variables for regression trees, and sqrt(p) variables for classification trees

For the importance measurements,  The former is based
upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The latter is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees 
```{r}
set.seed(1)

# build random forest classifier
rf.boston = randomForest(medv~.,data = Boston, subset = train,
                         mtry = 6, importance  = TRUE)
yhat.rf = predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf-boston.test)^2)

#outputting importance of each variable
importance(rf.boston)
varImpPlot(rf.boston)
```

Using the gbm package, we can fit booston regression trees to the boston daaset
if regression, do distribution = "gaussian"
if classification, do distribution = "bernoulli"

The argument n.trees=5000 indicates that we want 5000 trees, and the option interaction.depth=4 limits the depth of each tree.

We can produce partial dependence plots for lstate and rm, which show the marginal effect of the selected variables on the response after integrating out the other variables
default shrinkage value is .001
```{r}
library(gbm)
set.seed(1)
boost.boston = gbm(medv~.,data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = .2, verbose = F)

# lstat and rel.inf are by far our most useful predictors
summary(boost.boston)

# construct partial dependence plots 
par(mfrow = c(1,2))
plot(boost.boston,i = "rm")
plot(boost.boston, i ="lstat")

#predicting medv using the boosted model
# Test MSE comparable to that of RF model, and better than bagging model.
yhat.boost = predict(boost.boston,newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost-boston.test)^2)
```

