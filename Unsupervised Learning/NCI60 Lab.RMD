---
title: "ISLR Chapter 10 Lab 3"
output: html_notebook
---

This is Lab 3 of 3 for ISLR's Chapter 10 Lab Section
"Unsupervised techniques are often used in the analysis of genomic data.
In particular, PCA and hierarchical clustering are popular tools. We illustrate these techniques on the NCI60 cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines."

```{r}
library(ISLR)
nci.labs = NCI60$labs
nci.data = NCI60$data
# Get dimensions of data
dim(nci.data)

# Examining the cancer types for the cell lines
nci.labs[1:4]
table(nci.labs)

# Performing PCA on NCI60 data, but we don't necessarily have to scale the genes
pr.out  = prcomp(nci.data, scale = TRUE)

 # Functions assigns a color to each of the 64 cell lines, based on the cancer type to which it corresponds
# rainbow takes a positive integer as an argument, and returns a vector contianing that number of distinct colors
Cols = function(vec) {
  cols = rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

par(mfrow = c(1,2))
plot(pr.out$x[,1:2], col = Cols(nci.labs), pch = 19, xlab = "Z1", ylab = "Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19, xlab="Z1",ylab="Z3")

# We can obtain a summary of PVE of the first few principal components using the summary() method
summary(pr.out)

# Plotting PVE
pve = 100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow = c(1,2))
plot(pve , type="o", ylab="PVE", xlab=" Principal Component",
col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="
Principal Component ", col="brown3")
```

Next section deals with clustering the observations of the NCI60 Data
```{r}
sd.data = scale(nci.data)
par(mfrow = c(1,3))
data.dist = dist(sd.data)

# Hierarchical clustering is performed on the observations using complete, single, and average linkage. Euclidean distance is used as the dissimilarity measurement
# Single leakage yielded trailing clusters
# Cell lines within a single cancer type do tend to cluster together, but the clustering isn't perfect
plot(hclust(data.dist),labels = nci.labs, main = "Complete Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"),labels = nci.labs, main = "Average Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"),labels = nci.labs, main = "Single Linkage", xlab = "", sub = "", ylab = "")

# Complete linkage hierarchical clustering is used for the analysis that follows
# Dendrogram is cut so that four clusters remain
hc.out = hclust(dist(sd.data))
hc.clusters = cutree(hc.out,4)
table(hc.clusters,nci.labs)

# Cut on the dendrogram is plotted
par(mfrow = c(1,1))
plot(hc.out, labels = nci.labs)
abline(h=139, col = "red")
```

Next section involves combining PCA, K-means, and hierarchical clustering
```{r}
# Comparing K-means when K=4 to the hierarchical clustering
set.seed(2)
km.out = kmeans(sd.data,4,nstart = 20)
km.clusters = km.out$cluster

# The clusters are somewhat different, but cluster 2 in K-means is identical to cluster 3 in hierarchical clustering.
table(km.clusters, hc.clusters)

# Perform Hierarchical clustering on the first few principal component score vectors 
hc.out = hclust(dist(pr.out$x[,1:5]))
plot(hc.out,labels = nci.labs, main = "Hier. Cluster. on First Five Score Vectors")

# Results are different from the ones that we obtained when we performed hierarchical clustering on the full dataset 
# PCA can be used to denoise portions on the data
table(cutree(hc.out,4),nci.labs)
```

