---
title: "R Notebook"
output: html_notebook
---


Here, we split get a random sample of a data that'll be the training data, and we fit a linear regression and a polynomial regression
```{r}
library(ISLR)
library(boot)
set.seed(2)
train = sample(392,196)
lm.fit = lm(mpg~horsepower,data= Auto,subset = train)
lm_squared = lm(mpg~poly(horsepower,2), data = Auto, subset = train)
lm_cubed = lm(mpg~poly(horsepower,3), data = Auto, subset = train)
```
we estimate the MSE of the various fits, observing that the cubic fit has the lowest MSE of the three when set.seed() is 1. If we change set.seed, we get different results though, where the squared function may be better
```{r}
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
mean((mpg-predict(lm_squared,Auto))[-train]^2)
mean((mpg-predict(lm_cubed,Auto))[-train]^2)
detach(Auto)

```
glm function can create linear models
cv.glm functions produces a delta vector containing the leave-one-out cross validation results
Here, we loop through five degree fits to see which one corresponds to the lowest drop in MSE
Sharp drop going from linear to uadratic, but not much after that
```{r}
cv.error=rep(0,5)
for (i in 1:5){
 glm.fit=glm(mpg???poly(horsepower ,i),data=Auto)
 cv.error[i]=cv.glm(Auto ,glm.fit)$delta[1]
}
cv.error

```

K-fold CV is shown below, and still little evidence that higher degree polynomials beyong the squared line have a significant effect on reducing the MSE, with much less computation time
```{r}
set.seed(17)
cv.error.10 = rep(0,10)
for (i in 1:10) {
  glm.fit = glm(mpg~poly(horsepower,i),data = Auto)
  cv.error.10[i] = cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
```


Bootstrap steps in R: create a function, alpha.fn(), which takes as input the (X, Y ) data as well as a vector indicating which observations should be used to estimate ??.  The function then outputs the estimate for ?? based on the selected observations.
The next command uses the sample() function to randomly select 100 observations from the range 1 to 100, with replacement. 
boot() function can automate the rest
Here, we'll be using the Portfolio Dataset from the ISLR library, and perform a boostrap of size 1000
```{r}
alpha.fn = function(data, index) {
  X = data$X[index]
  Y = data$Y[index]
  return ((var(Y)-cov(X,Y))/(var(X)+var(Y) -2*cov(X,Y)))
}
boot(Portfolio, alpha.fn, R=1000)
```

boot.fn()  takes in the Auto data
set as well as a set of indices for the observations, and returns the intercept
and slope estimates for the linear regression model
The boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement.
Indicates that the bootstrap estimates for the standard errors of the parameters are .879 and .007 respectively 
Below the bootstrap output is the linear regression's computer standard errors, which are not as good as the boostrap estimates because they rely on cetrain assumptions regarding noise variance
```{r}
boot.fn = function(data,index) {
  return(coef(lm(mpg???horsepower ,data=data , subset=index)))
}
 boot.fn(Auto ,1:392)
 boot.fn(Auto,sample(392,392,replace = T))
 boot(Auto,boot.fn,1000)
 summary(lm(mpg~horsepower, data = Auto))
```


Below we compute the boostrap standard error estimates and the standard linear regression error estimates that result from fitting a quadratic model of the data.
Since this model is a more appropriate fit, there is now a better correspondance between the bootstrap estimates and the summary estimates of the standard errors of the three parameters.
```{r}
boot.fn = function(data,index) {
  return(coef(lm(mpg~horsepower + I(horsepower^2), data = data, subset = index)))
}
set.seed(1)
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower + I(horsepower^2), data = Auto))$coef
```

